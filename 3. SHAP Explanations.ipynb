{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Citations\n",
    "# Ridley, R., He, L., Dai, X., Huang, S., & Chen, J. (2020). Prompt agnostic essay scorer: a domain generalization approach to cross-prompt automated essay scoring. arXiv preprint arXiv:2008.01441.\n",
    "# Pethani, M. (2019) Automated Essay Scoring: Kaggle Competition â€” End to End Project Implementation. Medium. Retrieved from https://medium.com/@mayurmorin/automated-essay-scoring-kaggle-competition-end-to-end-project-implementation-part-1-b75a043903c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import readability\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.models\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from itertools import chain, combinations\n",
    "ignore_warnings = True\n",
    "if ignore_warnings:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 100 ['Kincaid', 'complex_words', 'type_token_ratio', 'words', 'wordtypes', 'subordination', 'conjunction', 'preposition']\n"
     ]
    }
   ],
   "source": [
    "#Load Settings\n",
    "loadName = 'DENSE_3004'\n",
    "includesGrades = True\n",
    "maxSentences = 15\n",
    "\n",
    "#Load Models\n",
    "lstm_model = tf.keras.models.load_model(loadName + '.h5')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(loadName + '_voc.bin', binary=True)\n",
    "\n",
    "#Load Settings\n",
    "settingsdf = pd.read_csv(loadName + '_settings.csv')\n",
    "print(settingsdf['num_features'][0], settingsdf['MAX_SENTLEN'][0], settingsdf['keepCats'][0])\n",
    "num_features = settingsdf['num_features'][0]\n",
    "MAX_SENTLEN = settingsdf['MAX_SENTLEN'][0]\n",
    "keepCats = ast.literal_eval(settingsdf['keepCats'][0])\n",
    "\n",
    "#Load Test Data\n",
    "X = pd.read_csv('./Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust dataframe\n",
    "OGcount = len(X)\n",
    "if not includesGrades:\n",
    "    X['Grade'] = -1\n",
    "X[\"nonseq_input\"] = X.apply(lambda x: [x[\"Grade\"]] + [x[\"Grade\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate all possible Coalitions of sentences                  \n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "for index, row in X.iterrows():\n",
    "    essay = row['Text']\n",
    "    grade = row['Grade']\n",
    "    trackIndex = row['Index']\n",
    "    \n",
    "    sentences = re.split(r'(\\? |! |\\. )', essay)\n",
    "    lastS = sentences[-1]\n",
    "    sentences = [\"\".join(pair) for pair in zip(sentences[::2], sentences[1::2])] \n",
    "    sentences = sentences + [lastS]\n",
    "\n",
    "\n",
    "    while len(sentences) > maxSentences:\n",
    "        lengths = [len(x)+len(sentences[i+1]) for i,x in enumerate(sentences) if i < len(sentences)-1]\n",
    "        indexMin = np.argmin(lengths)\n",
    "        newList = []\n",
    "        for i,s in enumerate(sentences):\n",
    "            if i == indexMin:\n",
    "                newItem = sentences[i] + sentences[i+1]\n",
    "                newList.append(newItem)\n",
    "            elif i == (indexMin+1):\n",
    "                continue\n",
    "            else:\n",
    "                newList.append(s)\n",
    "        sentences = newList\n",
    "\n",
    "    all = list(np.arange(len(sentences)))\n",
    "    coalitions = list(powerset(all))\n",
    "\n",
    "    oneOut = []\n",
    "    inTrack = []\n",
    "    for indices in coalitions:\n",
    "        newText = [sentences[x] for x in indices]\n",
    "        flatList = '.'.join(newText)\n",
    "        inTrack.append(indices)\n",
    "        oneOut.append(flatList)\n",
    "\n",
    "    d = {'essay_id': trackIndex, 'sentence_out': inTrack, 'Text': oneOut,'Grade': grade}\n",
    "    \n",
    "    df = pd.DataFrame(data=d)\n",
    "    if index == 0:\n",
    "        Xs = df[1:]\n",
    "    else:\n",
    "        Xs = pd.concat([Xs, df[1:]], ignore_index=True, axis=0)\n",
    "        \n",
    "Xs['Set'] = -1\n",
    "Xs[\"nonseq_input\"] = Xs.apply(lambda x: [x[\"Set\"]] + [x[\"Set\"]], axis=1)\n",
    "Xs\n",
    "X = Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions Word2Vec (Pethani, M., 2019)\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.get_vector(word))        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Features Functions (Ridley et al., 2020)\n",
    "def replace_url(text):\n",
    "    replaced_text = re.sub('(http[s]?://)?((www)\\.)?([a-zA-Z0-9]+)\\.{1}((com)(\\.(cn))?|(org))', '<url>', text)\n",
    "    return replaced_text\n",
    "\n",
    "def tokenize(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token == '@' and (index+1) < len(tokens):\n",
    "            tokens[index+1] = '@' + re.sub('[0-9]+.*', '', tokens[index+1])\n",
    "            tokens.pop(index)\n",
    "    return tokens\n",
    "\n",
    "def shorten_sentence(sent, max_sentlen):\n",
    "    new_tokens = []\n",
    "    sent = sent.strip()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    if len(tokens) > max_sentlen:\n",
    "        split_keywords = ['because', 'but', 'so', 'You', 'He', 'She', 'We', 'It', 'They', 'Your', 'His', 'Her']\n",
    "        k_indexes = [i for i, key in enumerate(tokens) if key in split_keywords]\n",
    "        processed_tokens = []\n",
    "        if not k_indexes:\n",
    "            num = len(tokens) / max_sentlen\n",
    "            num = int(round(num))\n",
    "            k_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "        processed_tokens.append(tokens[0:k_indexes[0]])\n",
    "        len_k = len(k_indexes)\n",
    "        for j in range(len_k-1):\n",
    "            processed_tokens.append(tokens[k_indexes[j]:k_indexes[j+1]])\n",
    "        processed_tokens.append(tokens[k_indexes[-1]:])\n",
    "\n",
    "        for token in processed_tokens:\n",
    "            if len(token) > max_sentlen:\n",
    "                num = len(token) / max_sentlen\n",
    "                num = int(np.ceil(num))\n",
    "                s_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "                len_s = len(s_indexes)\n",
    "                new_tokens.append(token[0:s_indexes[0]])\n",
    "                for j in range(len_s-1):\n",
    "                    new_tokens.append(token[s_indexes[j]:s_indexes[j+1]])\n",
    "                new_tokens.append(token[s_indexes[-1]:])\n",
    "\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "    else:\n",
    "        return [tokens]\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def tokenize_to_sentences(text, max_sentlength, create_vocab_flag=False):\n",
    "    sents = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', text)\n",
    "    processed_sents = []\n",
    "    for sent in sents:\n",
    "        if re.search(r'(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent):\n",
    "            s = re.split(r'(?=.{2,})(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent)\n",
    "            ss = \" \".join(s)\n",
    "            ssL = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', ss)\n",
    "\n",
    "            processed_sents.extend(ssL)\n",
    "        else:\n",
    "            processed_sents.append(sent)\n",
    "\n",
    "    if create_vocab_flag:\n",
    "        sent_tokens = [tokenize(sent) for sent in processed_sents]\n",
    "        tokens = [w for sent in sent_tokens for w in sent]\n",
    "        return tokens\n",
    "\n",
    "    sent_tokens = []\n",
    "    for sent in processed_sents:\n",
    "        shorten_sents_tokens = shorten_sentence(sent, max_sentlength)\n",
    "        sent_tokens.extend(shorten_sents_tokens)\n",
    "    return sent_tokens\n",
    "\n",
    "def text_tokenizer(text, replace_url_flag=True, tokenize_sent_flag=True, create_vocab_flag=False):\n",
    "    text = replace_url(text)\n",
    "    text = text.replace(u'\"', u'')\n",
    "    if \"...\" in text:\n",
    "        text = re.sub(r'\\.{3,}(\\s+\\.{3,})*', '...', text)\n",
    "    if \"??\" in text:\n",
    "        text = re.sub(r'\\?{2,}(\\s+\\?{2,})*', '?', text)\n",
    "    if \"!!\" in text:\n",
    "        text = re.sub(r'\\!{2,}(\\s+\\!{2,})*', '!', text)\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    if tokenize_sent_flag:\n",
    "        text = \" \".join(tokens)\n",
    "        sent_tokens = tokenize_to_sentences(text, MAX_SENTLEN, create_vocab_flag)\n",
    "        return sent_tokens\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "664477 / 664478/ 664478 / 664478 / 664478 664478 664478 / 664478/ 664478664478/ 664478664478/ 664478 / 664478 / 664478/ 664478/ 664478 / 664478/ 664478 / 664478 / 664478 664478664478/ 664478/ 664478 / 664478 / 664478/ 664478 / 664478\r"
     ]
    }
   ],
   "source": [
    "#Generate Writing Features (Ridley et al., 2020)\n",
    "i_ = 0\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "for index, row in X.iterrows():\n",
    "    if i_ % 1000:\n",
    "        print(index,'/', len(X), end='\\r')\n",
    "    content = row['Text']\n",
    "    score = row['Grade']\n",
    "\n",
    "    sent_tokens = text_tokenizer(content, replace_url_flag=True, tokenize_sent_flag=True)\n",
    "    sentences = [' '.join(sent) + '\\n' for sent in sent_tokens]\n",
    "    sentences = ''.join(sentences)\n",
    "    readability_scores = readability.getmeasures(sentences, lang='en')\n",
    "    \n",
    "    features = []\n",
    "    cats = []\n",
    "    #keepCats =['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word', 'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'directspeech_ratio', 'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc', 'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization', 'pronoun', 'interrogative', 'article', 'subordination', 'conjunction', 'preposition']\n",
    "\n",
    "    for cat in readability_scores.keys():\n",
    "        for subcat in readability_scores[cat].keys():\n",
    "            if subcat in keepCats:\n",
    "                cats.append(subcat)\n",
    "                ind_score = readability_scores[cat][subcat]\n",
    "                features.append(ind_score)\n",
    "\n",
    "        # find those words that may be misspelled\n",
    "    sentences = sentences.replace('\\n', ' ').replace('\\r', '').replace('etc', '')\n",
    "    words = sentences.split(\" \")\n",
    "    words = [x for x in words if '\\'' not in x and len(x)>3]\n",
    "    misspelled = spell.unknown(words)\n",
    "    features.append(len(misspelled))\n",
    "\n",
    "    X.at[i_, 'nonseq_input' ] = features\n",
    "    i_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20765/20765 [==============================] - 33s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predicting Test Set\n",
    "test_X_nonseq = np.asarray(list(X[\"nonseq_input\"]))\n",
    "test_essays = X['Text']\n",
    "\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "y_pred = lstm_model.predict([testDataVecs, test_X_nonseq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Predictions to Df\n",
    "y_pred = np.nan_to_num(y_pred)\n",
    "X['predGrade'] = y_pred * 10\n",
    "X[['Text', 'predGrade']].to_csv('nan.csv')\n",
    "\n",
    "X.predGrade = X.predGrade.round(3)\n",
    "X = X.rename(columns={\"sentence_out\": \"c_set\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 / 50 | 11 / 12\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>item</th>\n",
       "      <th>Marginal</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>575</td>\n",
       "      <td>0</td>\n",
       "      <td>0.307852</td>\n",
       "      <td>Libraries have always had interesting material...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>575</td>\n",
       "      <td>1</td>\n",
       "      <td>0.288676</td>\n",
       "      <td>Libraries are one of the best places to find t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>575</td>\n",
       "      <td>2</td>\n",
       "      <td>0.389540</td>\n",
       "      <td>I do not believe that libraries shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>575</td>\n",
       "      <td>3</td>\n",
       "      <td>0.217903</td>\n",
       "      <td>Thats all that libraries like are people inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>575</td>\n",
       "      <td>4</td>\n",
       "      <td>0.325146</td>\n",
       "      <td>If some people find different books offensive ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  item  Marginal                                           sentence\n",
       "1       575     0  0.307852  Libraries have always had interesting material...\n",
       "2       575     1  0.288676  Libraries are one of the best places to find t...\n",
       "3       575     2  0.389540           I do not believe that libraries shoul...\n",
       "4       575     3  0.217903  Thats all that libraries like are people inter...\n",
       "5       575     4  0.325146  If some people find different books offensive ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate SHAP values based on predictions (Brute-Force)\n",
    "d = {'essay_id': -1,'item': -1, 'Marginal': -1, 'sentence': ['a']}\n",
    "mdf = pd.DataFrame(data=d)\n",
    "\n",
    "g_ = 0\n",
    "for essayid in X['essay_id'].unique():  \n",
    "    #Track Progress\n",
    "    g_ += 1\n",
    "\n",
    "    #Loc values of specific essay\n",
    "    df = X.loc[X['essay_id'] == essayid]\n",
    "    all = list(df.iloc[-1]['c_set'])\n",
    "    coalitions = list(powerset(all))[1:]\n",
    "    i_ = 0\n",
    "\n",
    "    #Calculate contribution of sentence X\n",
    "    for item in all: \n",
    "        print(g_ , '/', len(X['essay_id'].unique()), '|', i_, '/', len(all), end= '\\r')\n",
    "        cumWeight = 0\n",
    "        marginal = 0\n",
    "        for coal in coalitions:\n",
    "            coalTemp = list(coal)\n",
    "            if item in coal:\n",
    "                removed_element = coalTemp.pop(coal.index(item))\n",
    "                if len(coalTemp) == 0:\n",
    "                    p = len(all)\n",
    "                    s = len(coalTemp)\n",
    "                    weight = round((math.factorial(s) * math.factorial(p - s - 1)) / math.factorial(p), 18)\n",
    "                else:\n",
    "                    p = len(all)\n",
    "                    s = len(coalTemp)\n",
    "                    weight = round((math.factorial(s) * math.factorial(p - s - 1)) / math.factorial(p), 18)\n",
    "                    marginal += weight * (df.loc[(df['c_set'] == coal)]['predGrade'].item() - df.loc[(df['c_set'] == tuple(coalTemp))]['predGrade'].item())\n",
    "\n",
    "        text = df.loc[(df['c_set'] == tuple([item]))]['Text'].item()\n",
    "        d = {'essay_id': essayid, 'item': [item], 'Marginal': [marginal], 'sentence': text} \n",
    "        tdf = pd.DataFrame(data=d)\n",
    "        mdf = pd.concat([mdf, tdf], ignore_index=True, axis=0)\n",
    "        mdf.to_csv('./Data/InspectSHAP.csv')\n",
    "        i_ += 1\n",
    "        \n",
    "mdf = mdf[1:]\n",
    "mdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize for words in the sentence\n",
    "def calculate_score_per_word(row):\n",
    "    text = row['sentence']\n",
    "    word_count = len(text.split())\n",
    "    grade = row['Marginal']\n",
    "    score_per_word = grade / word_count if word_count != 0 else 0\n",
    "    return score_per_word\n",
    "\n",
    "# Apply the function to each row\n",
    "mdf['Marginal_per_word'] = mdf.apply(calculate_score_per_word, axis=1)\n",
    "mdf.to_csv('./Data/InspectSHAP.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
