{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Citations\n",
    "# Ridley, R., He, L., Dai, X., Huang, S., & Chen, J. (2020). Prompt agnostic essay scorer: a domain generalization approach to cross-prompt automated essay scoring. arXiv preprint arXiv:2008.01441.\n",
    "# Pethani, M. (2019) Automated Essay Scoring: Kaggle Competition â€” End to End Project Implementation. Medium. Retrieved from https://medium.com/@mayurmorin/automated-essay-scoring-kaggle-competition-end-to-end-project-implementation-part-1-b75a043903c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oscar\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import readability\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dropout, Dense\n",
    "from tqdm.keras import TqdmCallback\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Settings\n",
    "testSize = 0.1      #Validation set\n",
    "seed = 42           #Seed\n",
    "numEpochs = 500     #Epochs\n",
    "batchSize = 64      #Batch Size\n",
    "ignore_warnings = True\n",
    "\n",
    "#Word vector settings\n",
    "num_features =  500  #Dimension of the word vector\n",
    "min_word_count = 5   #Mininum recurrence of word to be included\n",
    "num_workers = 4\n",
    "context = 30\n",
    "downsampling = 1e-3\n",
    "MAX_SENTLEN = 100\n",
    "\n",
    "#Writing features to use\n",
    "keepCats =  ['Kincaid' , 'complex_words', 'type_token_ratio', 'words', 'wordtypes', 'subordination', 'conjunction', 'preposition'] \n",
    "\n",
    "#File Names\n",
    "saveName = 'DENSE_3004'\n",
    "X = pd.read_csv('./Data/train.csv')\n",
    "\n",
    "#Save settings\n",
    "dsettings = {'num_features': num_features,'MAX_SENTLEN': MAX_SENTLEN, 'keepCats': [keepCats]}\n",
    "sdf = pd.DataFrame(data=dsettings)\n",
    "sdf.to_csv(saveName+ '_settings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore warnings\n",
    "if ignore_warnings:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Features Functions (Ridley et al., 2020)\n",
    "X[\"nonseq_input\"] = X.apply(lambda x: [x[\"Grade\"]] + [x[\"Grade\"]], axis=1)\n",
    "\n",
    "def replace_url(text):\n",
    "    replaced_text = re.sub('(http[s]?://)?((www)\\.)?([a-zA-Z0-9]+)\\.{1}((com)(\\.(cn))?|(org))', '<url>', text)\n",
    "    return replaced_text\n",
    "\n",
    "def tokenize(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token == '@' and (index+1) < len(tokens):\n",
    "            tokens[index+1] = '@' + re.sub('[0-9]+.*', '', tokens[index+1])\n",
    "            tokens.pop(index)\n",
    "    return tokens\n",
    "\n",
    "def shorten_sentence(sent, max_sentlen):\n",
    "    new_tokens = []\n",
    "    sent = sent.strip()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    if len(tokens) > max_sentlen:\n",
    "        split_keywords = ['because', 'but', 'so', 'You', 'He', 'She', 'We', 'It', 'They', 'Your', 'His', 'Her']\n",
    "        k_indexes = [i for i, key in enumerate(tokens) if key in split_keywords]\n",
    "        processed_tokens = []\n",
    "        if not k_indexes:\n",
    "            num = len(tokens) / max_sentlen\n",
    "            num = int(round(num))\n",
    "            k_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "        processed_tokens.append(tokens[0:k_indexes[0]])\n",
    "        len_k = len(k_indexes)\n",
    "        for j in range(len_k-1):\n",
    "            processed_tokens.append(tokens[k_indexes[j]:k_indexes[j+1]])\n",
    "        processed_tokens.append(tokens[k_indexes[-1]:])\n",
    "\n",
    "        for token in processed_tokens:\n",
    "            if len(token) > max_sentlen:\n",
    "                num = len(token) / max_sentlen\n",
    "                num = int(np.ceil(num))\n",
    "                s_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "                len_s = len(s_indexes)\n",
    "                new_tokens.append(token[0:s_indexes[0]])\n",
    "                for j in range(len_s-1):\n",
    "                    new_tokens.append(token[s_indexes[j]:s_indexes[j+1]])\n",
    "                new_tokens.append(token[s_indexes[-1]:])\n",
    "\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "    else:\n",
    "        return [tokens]\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def tokenize_to_sentences(text, max_sentlength, create_vocab_flag=False):\n",
    "    sents = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', text)\n",
    "    processed_sents = []\n",
    "    for sent in sents:\n",
    "        if re.search(r'(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent):\n",
    "            s = re.split(r'(?=.{2,})(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent)\n",
    "            ss = \" \".join(s)\n",
    "            ssL = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', ss)\n",
    "\n",
    "            processed_sents.extend(ssL)\n",
    "        else:\n",
    "            processed_sents.append(sent)\n",
    "\n",
    "    if create_vocab_flag:\n",
    "        sent_tokens = [tokenize(sent) for sent in processed_sents]\n",
    "        tokens = [w for sent in sent_tokens for w in sent]\n",
    "        return tokens\n",
    "\n",
    "    sent_tokens = []\n",
    "    for sent in processed_sents:\n",
    "        shorten_sents_tokens = shorten_sentence(sent, max_sentlength)\n",
    "        sent_tokens.extend(shorten_sents_tokens)\n",
    "    return sent_tokens\n",
    "\n",
    "def text_tokenizer(text, replace_url_flag=True, tokenize_sent_flag=True, create_vocab_flag=False):\n",
    "    text = replace_url(text)\n",
    "    text = text.replace(u'\"', u'')\n",
    "    if \"...\" in text:\n",
    "        text = re.sub(r'\\.{3,}(\\s+\\.{3,})*', '...', text)\n",
    "    if \"??\" in text:\n",
    "        text = re.sub(r'\\?{2,}(\\s+\\?{2,})*', '?', text)\n",
    "    if \"!!\" in text:\n",
    "        text = re.sub(r'\\!{2,}(\\s+\\!{2,})*', '!', text)\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    if tokenize_sent_flag:\n",
    "        text = \" \".join(tokens)\n",
    "        sent_tokens = tokenize_to_sentences(text, MAX_SENTLEN, create_vocab_flag)\n",
    "        return sent_tokens\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number features: 11\n",
      "Categories: ['Kincaid', 'type_token_ratio', 'words', 'wordtypes', 'complex_words', 'conjunction', 'preposition', 'subordination', 'conjunction', 'preposition']\n"
     ]
    }
   ],
   "source": [
    "#Generate Writing Features (Ridley et al., 2020)\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "i_ = 0\n",
    "for index, row in X.iterrows():\n",
    "    content = row['Text']\n",
    "    score = row['Grade']\n",
    "\n",
    "    sent_tokens = text_tokenizer(content, replace_url_flag=True, tokenize_sent_flag=True)\n",
    "    sentences = [' '.join(sent) + '\\n' for sent in sent_tokens]\n",
    "    sentences = ''.join(sentences)\n",
    "    readability_scores = readability.getmeasures(sentences, lang='en')\n",
    "    \n",
    "    features = []\n",
    "    cats = []\n",
    "    #keepCats = ['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word', 'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'directspeech_ratio', 'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc', 'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization', 'pronoun', 'interrogative', 'article', 'subordination', 'conjunction', 'preposition']\n",
    "\n",
    "    for cat in readability_scores.keys():\n",
    "        for subcat in readability_scores[cat].keys():\n",
    "            if subcat in keepCats:\n",
    "                cats.append(subcat)\n",
    "                ind_score = readability_scores[cat][subcat]\n",
    "                features.append(ind_score)\n",
    "                \n",
    "    # find those words that may be misspelled\n",
    "    sentences = sentences.replace('\\n', ' ').replace('\\r', '').replace('etc', '')\n",
    "    words = sentences.split(\" \")\n",
    "    words = [x for x in words if '\\'' not in x and len(x)>3]\n",
    "    misspelled = spell.unknown(words)\n",
    "    features.append(len(misspelled))\n",
    "\n",
    "    X.at[i_, 'nonseq_input' ] = features\n",
    "    i_ += 1\n",
    "    \n",
    "print('Number features:', len(X.iloc[1]['nonseq_input']))\n",
    "print('Categories:', cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to extract word vectors (Pethani, M.,2019)\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv.get_vector(word))        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Neural network\n",
    "num_non_seq_features = len(X.iloc[1]['nonseq_input'])\n",
    "def get_model(num_features):\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    #Input\n",
    "    input_layer = Input(shape=(num_features, ))\n",
    "    \n",
    "    #First Layers\n",
    "    hidden_layer = Dense(num_features, activation='relu')(input_layer)\n",
    "    dropout_layer = Dropout(0.5)(hidden_layer)\n",
    "    hidden_layer = Dense(256, activation='relu')(dropout_layer)\n",
    "    dropout_layer = Dropout(0.5)(hidden_layer)\n",
    "    hidden_layer = Dense(64, activation='relu')(dropout_layer)\n",
    "\n",
    "    #Concatenate\n",
    "    non_seq_input = Input(shape=(num_non_seq_features,))\n",
    "    concat_layer = Concatenate()([dropout_layer, non_seq_input])\n",
    "    \n",
    "    #Second layers\n",
    "    hidden_layer = Dense(128, activation='relu')(concat_layer)\n",
    "    dropout_layer = Dropout(0.3)(hidden_layer)\n",
    "\n",
    "    #Output\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "    model = Model(inputs=[input_layer, non_seq_input], outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train/Validation Split\n",
    "y = X['Grade']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testSize, random_state=seed)\n",
    "\n",
    "train_X_nonseq = np.asarray(list(X_train[\"nonseq_input\"]))\n",
    "test_X_nonseq = np.asarray(list(X_test[\"nonseq_input\"]))\n",
    "train_essays = X_train['Text']\n",
    "test_essays = X_test['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Essays: 1575\n",
      "Test Essays: 175\n",
      "Training Word2Vec Model...\n",
      "Vocabulary Size: 3164\n",
      "Saved Word2Vec: DENSE_3004_voc.bin\n",
      "(1575, 500)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [01:04<00:00,  7.78epoch/s, loss=0.602, mae=0.0651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model: DENSE_3004.h5\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "print('Train Essays:', len(X_train))\n",
    "print('Test Essays:', len(X_test))\n",
    "\n",
    "sentences = []\n",
    "for essay in train_essays:\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "        \n",
    "print(\"Training Word2Vec Model...\")\n",
    "model = Word2Vec(sentences, workers=num_workers, vector_size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "model.init_sims(replace=True)\n",
    "model.wv.save_word2vec_format(saveName + '_voc.bin', binary=True)\n",
    "print('Vocabulary Size:', len(model.wv.index_to_key))\n",
    "print('Saved Word2Vec:', saveName + '_voc.bin')\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "\n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "print(trainDataVecs.shape)\n",
    "\n",
    "lstm_model = get_model(num_features)\n",
    "lstm_model.fit([trainDataVecs, train_X_nonseq], y_train, batch_size=batchSize, epochs=numEpochs, verbose=0, callbacks=[TqdmCallback(verbose=1)])\n",
    "\n",
    "print('Saved Model:', saveName + '.h5')\n",
    "lstm_model.save(saveName + '.h5')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#Predicting Validation Essays\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "#testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "y_pred = lstm_model.predict([testDataVecs, test_X_nonseq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN predictions: 0\n",
      "175\n",
      "MAE: 0.6638128166539329\n",
      "[[7.386829257011414, 6.0], [8.092232942581177, 9.0], [7.799837589263916, 8.0], [6.654412746429443, 6.0], [7.3316216468811035, 8.0], [6.957600712776184, 7.0], [7.610688209533691, 7.0], [7.364897727966309, 8.0], [8.478503227233887, 8.0], [8.042458295822144, 8.0], [7.087007164955139, 5.0], [6.671549677848816, 7.0], [6.897796988487244, 8.0], [6.955739855766296, 7.0], [7.606330513954163, 9.0], [5.678170323371887, 6.0], [5.470612645149231, 3.0], [7.103715538978577, 6.0], [7.26502537727356, 7.0], [7.212899923324585, 8.0], [7.175543904304504, 7.0], [5.862725377082825, 5.0], [7.857930064201355, 8.0], [7.1925950050354, 8.0], [5.244874358177185, 4.0], [3.3785131573677063, 6.0], [6.2250107526779175, 6.0], [6.938796043395996, 6.0], [5.68595290184021, 6.0], [8.300674557685852, 8.0], [6.568998098373413, 7.0], [7.437593340873718, 8.0], [7.832106351852417, 8.0], [7.519814968109131, 8.0], [6.985836029052734, 8.0], [7.067549228668213, 6.0], [7.05170214176178, 6.0], [8.516886234283447, 9.0], [6.089640855789185, 6.0], [7.40782618522644, 8.0], [7.4881696701049805, 8.0], [6.637033224105835, 6.0], [7.2163039445877075, 8.0], [7.701007723808289, 7.0], [5.530958771705627, 4.0], [6.516237854957581, 7.0], [7.8832948207855225, 8.0], [6.988098621368408, 7.0], [7.555076479911804, 8.0], [7.319706678390503, 7.0], [6.719829440116882, 8.0], [6.287575364112854, 6.0], [8.910226225852966, 9.0], [6.763107180595398, 7.0], [7.570703029632568, 6.0], [6.764963865280151, 6.0], [7.659091353416443, 8.0], [7.5860512256622314, 6.0], [5.903056859970093, 5.0], [5.409145951271057, 4.0], [6.62103533744812, 8.0], [7.077699899673462, 8.0], [7.056413888931274, 8.0], [7.835173606872559, 8.0], [6.58109188079834, 6.0], [8.144354820251465, 8.0], [6.850239038467407, 5.0], [5.189085006713867, 5.0], [7.378264665603638, 8.0], [4.641962051391602, 4.0], [8.087816834449768, 9.0], [7.526214122772217, 9.0], [6.871397495269775, 7.0], [6.521032452583313, 7.0], [6.836254000663757, 8.0], [6.8477267026901245, 7.0], [6.877245306968689, 6.0], [4.8005300760269165, 4.0], [7.682923674583435, 8.0], [7.141582369804382, 8.0], [5.065402984619141, 6.0], [6.018569469451904, 6.0], [5.794151425361633, 4.0], [7.1242886781692505, 7.0], [8.005264401435852, 8.0], [2.0617716014385223, 3.0], [7.103732228279114, 8.0], [6.575152277946472, 7.0], [8.06804358959198, 8.0], [6.981294751167297, 6.0], [6.054673194885254, 6.0], [6.903871297836304, 7.0], [7.20519483089447, 7.0], [7.276427745819092, 8.0], [7.14270293712616, 7.0], [7.740586996078491, 7.0], [6.884488463401794, 7.0], [8.317665457725525, 8.0], [6.711735129356384, 6.0], [6.75874650478363, 6.0], [7.31719970703125, 6.0], [7.789198160171509, 8.0], [7.406014800071716, 7.0], [5.432693362236023, 5.0], [6.072828769683838, 5.0], [5.8360666036605835, 6.0], [6.718764901161194, 7.0], [5.832778215408325, 4.0], [3.7749147415161133, 5.0], [7.34113872051239, 7.0], [5.133087635040283, 6.0], [7.869677543640137, 8.0], [4.848520457744598, 4.0], [7.597748637199402, 8.0], [7.3470622301101685, 8.0], [7.97474205493927, 8.0], [6.622371673583984, 5.0], [7.2378236055374146, 8.0], [7.070624828338623, 8.0], [7.407163977622986, 7.0], [7.858871817588806, 9.0], [6.987813115119934, 6.0], [7.648527026176453, 7.0], [7.782836556434631, 8.0], [8.086937665939331, 8.0], [6.389957666397095, 8.0], [6.863186955451965, 8.0], [6.9787681102752686, 7.0], [7.400306463241577, 8.0], [6.723036170005798, 7.0], [6.442203521728516, 6.0], [7.64723002910614, 8.0], [6.815656423568726, 6.0], [5.7680743932724, 6.0], [8.379996418952942, 8.0], [6.152845025062561, 6.0], [7.3345935344696045, 8.0], [6.57065749168396, 6.0], [8.075876235961914, 8.0], [7.102932929992676, 7.0], [7.28288471698761, 8.0], [6.968708634376526, 6.0], [7.700227499008179, 8.0], [7.618699669837952, 8.0], [7.3353493213653564, 7.0], [7.476415634155273, 8.0], [7.52280056476593, 9.0], [7.186201810836792, 8.0], [7.1632784605026245, 6.0], [7.441099286079407, 8.0], [7.578548789024353, 7.0], [7.334862947463989, 6.0], [8.07448148727417, 8.0], [7.3323118686676025, 8.0], [8.189713954925537, 8.0], [6.096909046173096, 8.0], [6.756129264831543, 6.0], [7.816360592842102, 8.0], [7.509474754333496, 8.0], [6.64164662361145, 6.0], [6.6846418380737305, 7.0], [7.2699809074401855, 8.0], [8.01073968410492, 7.0], [7.721624970436096, 8.0], [5.979785919189453, 6.0], [4.367308020591736, 5.0], [6.731443405151367, 8.0], [8.625779747962952, 9.0], [6.644685864448547, 6.0], [7.960227727890015, 7.0], [6.772270202636719, 6.0], [7.397478818893433, 8.0], [5.772906541824341, 5.0], [5.280816555023193, 5.0], [6.7701274156570435, 8.0]]\n"
     ]
    }
   ],
   "source": [
    "#MAE\n",
    "nanValuesTrack = len(y_pred)\n",
    "y_pred_clean = y_pred[~np.isnan(y_pred)]\n",
    "print('NaN predictions:', nanValuesTrack - len(y_pred_clean))\n",
    "print(len(y_pred_clean))\n",
    "\n",
    "inspectionList = []\n",
    "mae = 0\n",
    "for pred, test in zip(y_pred_clean,y_test.to_list()):\n",
    "        mae += np.abs(pred*10 - test*10)\n",
    "        inspectionList.append([pred*10, test*10])\n",
    "mae = mae/len(y_pred_clean)\n",
    "\n",
    "print('MAE:', mae)\n",
    "print(inspectionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 6), (8, 9), (8, 8), (7, 6), (7, 8), (7, 7), (8, 7), (7, 8), (8, 8), (8, 8), (7, 5), (7, 7), (7, 8), (7, 7), (8, 9), (6, 6), (5, 3), (7, 6), (7, 7), (7, 8), (7, 7), (6, 5), (8, 8), (7, 8), (5, 4), (3, 6), (6, 6), (7, 6), (6, 6), (8, 8), (7, 7), (7, 8), (8, 8), (8, 8), (7, 8), (7, 6), (7, 6), (9, 9), (6, 6), (7, 8), (7, 8), (7, 6), (7, 8), (8, 7), (6, 4), (7, 7), (8, 8), (7, 7), (8, 8), (7, 7), (7, 8), (6, 6), (9, 9), (7, 7), (8, 6), (7, 6), (8, 8), (8, 6), (6, 5), (5, 4), (7, 8), (7, 8), (7, 8), (8, 8), (7, 6), (8, 8), (7, 5), (5, 5), (7, 8), (5, 4), (8, 9), (8, 9), (7, 7), (7, 7), (7, 8), (7, 7), (7, 6), (5, 4), (8, 8), (7, 8), (5, 6), (6, 6), (6, 4), (7, 7), (8, 8), (2, 3), (7, 8), (7, 7), (8, 8), (7, 6), (6, 6), (7, 7), (7, 7), (7, 8), (7, 7), (8, 7), (7, 7), (8, 8), (7, 6), (7, 6), (7, 6), (8, 8), (7, 7), (5, 5), (6, 5), (6, 6), (7, 7), (6, 4), (4, 5), (7, 7), (5, 6), (8, 8), (5, 4), (8, 8), (7, 8), (8, 8), (7, 5), (7, 8), (7, 8), (7, 7), (8, 9), (7, 6), (8, 7), (8, 8), (8, 8), (6, 8), (7, 8), (7, 7), (7, 8), (7, 7), (6, 6), (8, 8), (7, 6), (6, 6), (8, 8), (6, 6), (7, 8), (7, 6), (8, 8), (7, 7), (7, 8), (7, 6), (8, 8), (8, 8), (7, 7), (7, 8), (8, 9), (7, 8), (7, 6), (7, 8), (8, 7), (7, 6), (8, 8), (7, 8), (8, 8), (6, 8), (7, 6), (8, 8), (8, 8), (7, 6), (7, 7), (7, 8), (8, 7), (8, 8), (6, 6), (4, 5), (7, 8), (9, 9), (7, 6), (8, 7), (7, 6), (7, 8), (6, 5), (5, 5), (7, 8)]\n",
      "QWK: 0.712123018238169\n"
     ]
    }
   ],
   "source": [
    "#QWK\n",
    "preds = y_pred_clean\n",
    "actuals = y_test.to_list()\n",
    "preds = [int(round(i*10)) for i in preds]\n",
    "actuals = [int(round(i*10)) for i in actuals]\n",
    "print(list(zip(preds,actuals)))\n",
    "print('QWK:', cohen_kappa_score(preds, actuals, weights='quadratic'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
