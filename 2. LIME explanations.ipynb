{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Citations\n",
    "# Ridley, R., He, L., Dai, X., Huang, S., & Chen, J. (2020). Prompt agnostic essay scorer: a domain generalization approach to cross-prompt automated essay scoring. arXiv preprint arXiv:2008.01441.\n",
    "# Pethani, M. (2019) Automated Essay Scoring: Kaggle Competition â€” End to End Project Implementation. Medium. Retrieved from https://medium.com/@mayurmorin/automated-essay-scoring-kaggle-competition-end-to-end-project-implementation-part-1-b75a043903c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import readability\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.models\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "ignore_warnings = True\n",
    "if ignore_warnings:\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import docx \n",
    "from docx.enum.text import WD_COLOR_INDEX \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 100 ['Kincaid', 'complex_words', 'type_token_ratio', 'words', 'wordtypes', 'subordination', 'conjunction', 'preposition']\n"
     ]
    }
   ],
   "source": [
    "#Load Settings\n",
    "loadName = 'DENSE_3004'\n",
    "includesGrades = True\n",
    "\n",
    "#Load Models\n",
    "lstm_model = tf.keras.models.load_model(loadName + '.h5')\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(loadName + '_voc.bin', binary=True)\n",
    "\n",
    "#Load Settings\n",
    "settingsdf = pd.read_csv(loadName + '_settings.csv')\n",
    "print(settingsdf['num_features'][0], settingsdf['MAX_SENTLEN'][0], settingsdf['keepCats'][0])\n",
    "num_features = settingsdf['num_features'][0]\n",
    "MAX_SENTLEN = settingsdf['MAX_SENTLEN'][0]\n",
    "keepCats = ast.literal_eval(settingsdf['keepCats'][0])\n",
    "\n",
    "#Load Test Data\n",
    "X = pd.read_csv('./Data/test.csv')\n",
    "Xtrain = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust Dataframe\n",
    "if not includesGrades:\n",
    "    X['Grade'] = -1\n",
    "    Xtrain['Grade'] = -1\n",
    "X[\"nonseq_input\"] = X.apply(lambda x: [x[\"Grade\"]] + [x[\"Grade\"]], axis=1)\n",
    "Xtrain[\"nonseq_input\"] = Xtrain.apply(lambda x: [x[\"Grade\"]] + [x[\"Grade\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate writing features for explanation categories\n",
    "explainCategories =  ['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', \n",
    " 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word', 'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', \n",
    " 'type_token_ratio', 'directspeech_ratio', 'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', \n",
    " 'complex_words', 'complex_words_dc', 'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization', 'pronoun', \n",
    " 'interrogative', 'article', 'subordination', 'conjunction', 'preposition'] \n",
    "\n",
    "for cat in explainCategories:\n",
    "    X[cat] = 0\n",
    "    Xtrain[cat] = 0\n",
    "\n",
    "X['spelling_mistakes'] = 0\n",
    "Xtrain['spelling_mistakes'] = 0\n",
    "\n",
    "X['word_count'] = 0\n",
    "Xtrain['word_count'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions Word2Vec (Pethani, M., 2019)\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.index_to_key)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.get_vector(word))        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Features Functions (Ridley et al., 2020)\n",
    "def replace_url(text):\n",
    "    replaced_text = re.sub('(http[s]?://)?((www)\\.)?([a-zA-Z0-9]+)\\.{1}((com)(\\.(cn))?|(org))', '<url>', text)\n",
    "    return replaced_text\n",
    "\n",
    "def tokenize(string):\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token == '@' and (index+1) < len(tokens):\n",
    "            tokens[index+1] = '@' + re.sub('[0-9]+.*', '', tokens[index+1])\n",
    "            tokens.pop(index)\n",
    "    return tokens\n",
    "\n",
    "def shorten_sentence(sent, max_sentlen):\n",
    "    new_tokens = []\n",
    "    sent = sent.strip()\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    if len(tokens) > max_sentlen:\n",
    "        split_keywords = ['because', 'but', 'so', 'You', 'He', 'She', 'We', 'It', 'They', 'Your', 'His', 'Her']\n",
    "        k_indexes = [i for i, key in enumerate(tokens) if key in split_keywords]\n",
    "        processed_tokens = []\n",
    "        if not k_indexes:\n",
    "            num = len(tokens) / max_sentlen\n",
    "            num = int(round(num))\n",
    "            k_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "        processed_tokens.append(tokens[0:k_indexes[0]])\n",
    "        len_k = len(k_indexes)\n",
    "        for j in range(len_k-1):\n",
    "            processed_tokens.append(tokens[k_indexes[j]:k_indexes[j+1]])\n",
    "        processed_tokens.append(tokens[k_indexes[-1]:])\n",
    "\n",
    "        for token in processed_tokens:\n",
    "            if len(token) > max_sentlen:\n",
    "                num = len(token) / max_sentlen\n",
    "                num = int(np.ceil(num))\n",
    "                s_indexes = [(i+1)*max_sentlen for i in range(num)]\n",
    "\n",
    "                len_s = len(s_indexes)\n",
    "                new_tokens.append(token[0:s_indexes[0]])\n",
    "                for j in range(len_s-1):\n",
    "                    new_tokens.append(token[s_indexes[j]:s_indexes[j+1]])\n",
    "                new_tokens.append(token[s_indexes[-1]:])\n",
    "\n",
    "            else:\n",
    "                new_tokens.append(token)\n",
    "    else:\n",
    "        return [tokens]\n",
    "\n",
    "    return new_tokens\n",
    "\n",
    "def tokenize_to_sentences(text, max_sentlength, create_vocab_flag=False):\n",
    "    sents = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', text)\n",
    "    processed_sents = []\n",
    "    for sent in sents:\n",
    "        if re.search(r'(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent):\n",
    "            s = re.split(r'(?=.{2,})(?<=\\.{1}|\\!|\\?|\\,)(@?[A-Z]+[a-zA-Z]*[0-9]*)', sent)\n",
    "            ss = \" \".join(s)\n",
    "            ssL = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s', ss)\n",
    "\n",
    "            processed_sents.extend(ssL)\n",
    "        else:\n",
    "            processed_sents.append(sent)\n",
    "\n",
    "    if create_vocab_flag:\n",
    "        sent_tokens = [tokenize(sent) for sent in processed_sents]\n",
    "        tokens = [w for sent in sent_tokens for w in sent]\n",
    "        return tokens\n",
    "\n",
    "    sent_tokens = []\n",
    "    for sent in processed_sents:\n",
    "        shorten_sents_tokens = shorten_sentence(sent, max_sentlength)\n",
    "        sent_tokens.extend(shorten_sents_tokens)\n",
    "    return sent_tokens\n",
    "\n",
    "def text_tokenizer(text, replace_url_flag=True, tokenize_sent_flag=True, create_vocab_flag=False):\n",
    "    text = replace_url(text)\n",
    "    text = text.replace(u'\"', u'')\n",
    "    if \"...\" in text:\n",
    "        text = re.sub(r'\\.{3,}(\\s+\\.{3,})*', '...', text)\n",
    "    if \"??\" in text:\n",
    "        text = re.sub(r'\\?{2,}(\\s+\\?{2,})*', '?', text)\n",
    "    if \"!!\" in text:\n",
    "        text = re.sub(r'\\!{2,}(\\s+\\!{2,})*', '!', text)\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    if tokenize_sent_flag:\n",
    "        text = \" \".join(tokens)\n",
    "        sent_tokens = tokenize_to_sentences(text, MAX_SENTLEN, create_vocab_flag)\n",
    "        return sent_tokens\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Writing Features (Ridley et al., 2020)\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()\n",
    "\n",
    "i_ = 0\n",
    "for index, row in X.iterrows():\n",
    "    content = row['Text']\n",
    "    score = row['Grade']\n",
    "\n",
    "    sent_tokens = text_tokenizer(content, replace_url_flag=True, tokenize_sent_flag=True)\n",
    "    sentences = [' '.join(sent) + '\\n' for sent in sent_tokens]\n",
    "    sentences = ''.join(sentences)\n",
    "    readability_scores = readability.getmeasures(sentences, lang='en')\n",
    "    \n",
    "    features = []\n",
    "    cats = []\n",
    "    #keepCats =['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word', 'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'directspeech_ratio', 'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc', 'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization', 'pronoun', 'interrogative', 'article', 'subordination', 'conjunction', 'preposition']\n",
    "\n",
    "    for cat in readability_scores.keys():\n",
    "        for subcat in readability_scores[cat].keys():\n",
    "            ind_score = readability_scores[cat][subcat]\n",
    "            if subcat in explainCategories:\n",
    "                X.at[i_, subcat] += ind_score\n",
    "            if subcat in keepCats:\n",
    "                cats.append(subcat)\n",
    "                features.append(ind_score)\n",
    "    # find those words that may be misspelled\n",
    "    sentences = sentences.replace('\\n', ' ').replace('\\r', '').replace('etc', '')\n",
    "    words = sentences.split(\" \")\n",
    "    X.at[i_, 'word_count'] = len(words)\n",
    "    words = [x for x in words if '\\'' not in x and len(x)>3]\n",
    "    misspelled = spell.unknown(words)\n",
    "    features.append(len(misspelled))\n",
    "    X.at[i_, 'spelling_mistakes'] = len(misspelled)\n",
    "\n",
    "    X.at[i_, 'nonseq_input' ] = features\n",
    "    i_ += 1\n",
    "\n",
    "\n",
    "g_ = 0\n",
    "for index, row in Xtrain.iterrows():\n",
    "    content = row['Text']\n",
    "    score = row['Grade']\n",
    "\n",
    "    sent_tokens = text_tokenizer(content, replace_url_flag=True, tokenize_sent_flag=True)\n",
    "    sentences = [' '.join(sent) + '\\n' for sent in sent_tokens]\n",
    "    sentences = ''.join(sentences)\n",
    "    readability_scores = readability.getmeasures(sentences, lang='en')\n",
    "    \n",
    "    features = []\n",
    "    cats = []\n",
    "    #keepCats =['Kincaid', 'ARI', 'Coleman-Liau', 'FleschReadingEase', 'GunningFogIndex', 'LIX', 'SMOGIndex', 'RIX', 'DaleChallIndex', 'characters_per_word', 'syll_per_word', 'words_per_sentence', 'sentences_per_paragraph', 'type_token_ratio', 'directspeech_ratio', 'characters', 'syllables', 'words', 'wordtypes', 'sentences', 'paragraphs', 'long_words', 'complex_words', 'complex_words_dc', 'tobeverb', 'auxverb', 'conjunction', 'pronoun', 'preposition', 'nominalization', 'pronoun', 'interrogative', 'article', 'subordination', 'conjunction', 'preposition']\n",
    "\n",
    "    for cat in readability_scores.keys():\n",
    "        for subcat in readability_scores[cat].keys():\n",
    "            ind_score = readability_scores[cat][subcat]\n",
    "            if subcat in explainCategories:\n",
    "                Xtrain.at[g_, subcat] += ind_score\n",
    "            if subcat in keepCats:\n",
    "                cats.append(subcat)\n",
    "                features.append(ind_score)\n",
    "                \n",
    "    # find those words that may be misspelled\n",
    "    sentences = sentences.replace('\\n', ' ').replace('\\r', '').replace('etc', '')\n",
    "    words = sentences.split(\" \")\n",
    "    Xtrain.at[g_, 'word_count'] = len(words)\n",
    "    words = [x for x in words if '\\'' not in x and len(x)>3]\n",
    "    misspelled = spell.unknown(words)\n",
    "    features.append(len(misspelled))\n",
    "    Xtrain.at[g_, 'spelling_mistakes'] = len(misspelled)\n",
    "\n",
    "    Xtrain.at[g_, 'nonseq_input' ] = features\n",
    "    g_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "#Predicting Test set and saving the scores\n",
    "test_X_nonseq = np.asarray(list(X[\"nonseq_input\"]))\n",
    "test_essays = X['Text']\n",
    "\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "#testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "y_pred = lstm_model.predict([testDataVecs, test_X_nonseq])\n",
    "\n",
    "d = {'Index': list(X['Index'].values), 'Score': [x[0]*10 for x in y_pred]}\n",
    "scoreDf = pd.DataFrame(data=d)\n",
    "scoreDf.to_csv('./Data/scoreDf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#Including the training data for explanations\n",
    "Xtrain = pd.concat((Xtrain, X))\n",
    "\n",
    "Xtrain_nonseq = np.asarray(list(Xtrain['nonseq_input']))\n",
    "Xtrain_essays = Xtrain['Text']\n",
    "\n",
    "clean_essays = []\n",
    "for essay_v in Xtrain_essays:\n",
    "    clean_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True ))\n",
    "\n",
    "DataVecs = getAvgFeatureVecs(clean_essays, model, num_features )\n",
    "DataVecs = np.array(DataVecs)\n",
    "y_pred_train = lstm_model.predict([DataVecs, Xtrain_nonseq])\n",
    "\n",
    "Xtrain['pGrade'] = y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add some custom explanation features\n",
    "\n",
    "#Argumentative words\n",
    "Xtrain['countWords_argumentation'] = Xtrain['Text'].apply(lambda x: x.count('because') + x.count('Because') + x.count('therefore') + x.count('Therefore') + x.count('but') + x.count('But') + x.count('believe')+ x.count('think') + x.count('perspective')+ x.count('then')+ x.count('Then')+ x.count('example'))\n",
    "\n",
    "#Prompt adherence\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(Xtrain['Text'])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, columns=Xtrain.index, index=Xtrain.index)\n",
    "average_similarity_per_text = np.mean(cosine_sim, axis=1)\n",
    "Xtrain['sim'] = average_similarity_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of writing features for groupings\n",
    "\n",
    "#Writing Quality\n",
    "Xtrain['spelling_mistakes'] = Xtrain['spelling_mistakes'] * -1\n",
    "Xtrain['spelling_mistakes'] = Xtrain['spelling_mistakes'] / Xtrain['word_count']\n",
    "Xtrain['Coleman-Liau'] = Xtrain['Coleman-Liau']\n",
    "\n",
    "#Word Usage\n",
    "Xtrain['wordtypes'] = Xtrain['wordtypes']\n",
    "Xtrain['complex_words'] = Xtrain['complex_words']  \n",
    "Xtrain['characters_per_word'] = Xtrain['characters_per_word']\n",
    "\n",
    "#Argumentation\n",
    "Xtrain['preposition'] = Xtrain['preposition'] \n",
    "Xtrain['complex_words'] = Xtrain['complex_words']\n",
    "\n",
    "#Prompt adherence\n",
    "Xtrain['sim'] = Xtrain['sim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank all variables column-wise for normalization\n",
    "for column in Xtrain.columns[6:]:\n",
    "    Xtrain[column] = Xtrain[column].rank(method='first')\n",
    "    Xtrain[column] = Xtrain[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Quality\n",
      "CW 0.45795265163147686\n",
      "SPEL 0.6448969356677785\n",
      "\n",
      "Word Usage\n",
      "TTR -0.359701944352452\n",
      "complex 0.847760113506208\n",
      "\n",
      "Argumentation\n",
      "PREP 0.6985830201387924\n",
      "ARG 0.29513676804632755\n",
      "\n",
      "Prompt adherence\n",
      "sim 0.42816066507016415\n"
     ]
    }
   ],
   "source": [
    "#Inspect variables selected\n",
    "print('Writing Quality')\n",
    "print('CW', Xtrain['pGrade'].corr(Xtrain['characters_per_word']))\n",
    "print('SPEL', Xtrain['pGrade'].corr(Xtrain['spelling_mistakes']))\n",
    "print()\n",
    "print('Word Usage')\n",
    "print('TTR', Xtrain['pGrade'].corr(Xtrain['type_token_ratio']))\n",
    "print('complex', Xtrain['pGrade'].corr(Xtrain['complex_words']))\n",
    "print()\n",
    "print('Argumentation')\n",
    "print('PREP', Xtrain['pGrade'].corr(Xtrain['preposition']))\n",
    "print('ARG', Xtrain['pGrade'].corr(Xtrain['countWords_argumentation']))\n",
    "print()\n",
    "print('Prompt adherence')\n",
    "print('sim', Xtrain['pGrade'].corr(Xtrain['sim']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WU 0.8532541284785863\n",
      "WQ 0.6919491503164783\n",
      "TL 0.42816066507016415\n",
      "ARG 0.5794891015400212\n"
     ]
    }
   ],
   "source": [
    "#Create interpretable groups\n",
    "d = {'Index': np.repeat(-1, len(X)), 'wordUsage': -1, 'writingQuality': -1, 'textLength': -1, 'argumentation': -1}\n",
    "\n",
    "Xtrain['wordUsage'] =  Xtrain['characters_per_word'] + Xtrain['wordtypes'] + Xtrain['complex_words'] \n",
    "Xtrain['writingQuality'] = Xtrain['Coleman-Liau']  + Xtrain['spelling_mistakes']\n",
    "Xtrain['textLength'] = Xtrain['sim'] \n",
    "Xtrain['argumentation'] = Xtrain['preposition'] + Xtrain['countWords_argumentation']\n",
    "\n",
    "print('WU', Xtrain['pGrade'].corr(Xtrain['wordUsage']))\n",
    "print('WQ', Xtrain['pGrade'].corr(Xtrain['writingQuality']))\n",
    "print('TL', Xtrain['pGrade'].corr(Xtrain['textLength']))\n",
    "print('ARG', Xtrain['pGrade'].corr(Xtrain['argumentation']))\n",
    "\n",
    "Xtrain = Xtrain[['Index','Text','Grade','nonseq_input', 'wordUsage', 'writingQuality', 'textLength', 'argumentation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank groups\n",
    "for column in Xtrain.columns[4:]:\n",
    "    Xtrain[column] = Xtrain[column].rank(method='first')\n",
    "    Xtrain[column] = Xtrain[column].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve test data\n",
    "X = Xtrain.loc[Xtrain['Index'].isin(X['Index'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find closest values\n",
    "def find_closest_values(array2D, differenceDf):\n",
    "    cvalues = [] #column values\n",
    "    indexSelection = []\n",
    "    \n",
    "    for h_, column in enumerate(differenceDf.columns[4:]):\n",
    "        cvalues.append(differenceDf[column].values)\n",
    "        \n",
    "    i_ = 0\n",
    "    for array in array2D:\n",
    "        result = 0\n",
    "        for i in range(len(cvalues)):\n",
    "            a = cvalues[i]\n",
    "            b = np.repeat((array[i]), len(cvalues[i]))\n",
    "            result += np.abs(a - b)\n",
    "\n",
    "        bestCandidates = np.where(result == result.min())\n",
    "        best = np.random.choice(bestCandidates[0])\n",
    "        best = differenceDf.iloc[best]['Index']\n",
    "        i_ += 1\n",
    "        indexSelection.append(best)\n",
    "        \n",
    "    return indexSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Explanations with LIME\n",
    "text_features = Xtrain[['Text', 'nonseq_input']].values\n",
    "custom_features = Xtrain[Xtrain.columns[4:]].values\n",
    "\n",
    "def predict_fn(data):\n",
    "\n",
    "    testCase = find_closest_values([data[0]], X)\n",
    "    Xtest = X.loc[X['Index'] == testCase[0]]\n",
    "\n",
    "    pertubations = data[1:]\n",
    "    mostSimilar = find_closest_values(pertubations, Xtrain)\n",
    "    XmostSimilar = pd.DataFrame()\n",
    "    for index in mostSimilar:\n",
    "        XmostSimilar = pd.concat([XmostSimilar, Xtrain.loc[Xtrain['Index'] == index]], ignore_index=True)\n",
    "    \n",
    "    Xcompute = pd.concat([Xtest, XmostSimilar], ignore_index=True, axis=0)\n",
    "    X_nonseq = np.asarray(list(Xcompute['nonseq_input']))\n",
    "    X_essays = Xcompute['Text']\n",
    "\n",
    "    clean_essays = []\n",
    "    for essay_v in X_essays:\n",
    "        clean_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True ))\n",
    "\n",
    "    DataVecs = getAvgFeatureVecs(clean_essays, model, num_features )\n",
    "    DataVecs = np.array(DataVecs)\n",
    "    y_pred_ = lstm_model.predict([DataVecs, X_nonseq])\n",
    "    return y_pred_*10\n",
    "catFeat = list(np.arange(len(Xtrain.columns[4:])))\n",
    "width = math.sqrt(len(Xtrain.columns[4:])) * 0.75 \n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(custom_features, feature_names=Xtrain.columns[4:], mode='regression', kernel_width=width, discretize_continuous= True, discretizer='quartile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essay: 0 / 50\n",
      "Sample: 575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.39825719500674334)\n",
      "item: ('textLength > 1350.25', 0.22537897713885613)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.18686204350897465)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.10488411836496435)\n",
      "Essay: 1 / 50\n",
      "Sample: 1512\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('writingQuality > 1350.25', 0.3986045731682742)\n",
      "item: ('argumentation <= 450.75', -0.3042128730104195)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.26611706495187015)\n",
      "item: ('textLength <= 450.75', -0.2565966708781153)\n",
      "Essay: 2 / 50\n",
      "Sample: 896\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7161786646621319)\n",
      "item: ('writingQuality <= 450.75', -0.43092314829901013)\n",
      "item: ('450.75 < textLength <= 900.50', -0.11469018699415322)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07698199567078882)\n",
      "Essay: 3 / 50\n",
      "Sample: 1349\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.746970968147116)\n",
      "item: ('textLength <= 450.75', -0.2911646261932919)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.1568624110129333)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.06998593315558058)\n",
      "Essay: 4 / 50\n",
      "Sample: 1795\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.723339378733856)\n",
      "item: ('writingQuality <= 450.75', -0.4302338927876161)\n",
      "item: ('argumentation <= 450.75', -0.2951010308287361)\n",
      "item: ('450.75 < textLength <= 900.50', -0.10768854014898369)\n",
      "Essay: 5 / 50\n",
      "Sample: 438\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('argumentation <= 450.75', -0.3283837855793949)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.19061017401458658)\n",
      "item: ('450.75 < textLength <= 900.50', -0.08200276489602953)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.06651539870472993)\n",
      "Essay: 6 / 50\n",
      "Sample: 1178\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('writingQuality > 1350.25', 0.41358851769787547)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.1825878485276193)\n",
      "item: ('450.75 < textLength <= 900.50', -0.06115614090042104)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.05250266414437234)\n",
      "Essay: 7 / 50\n",
      "Sample: 1468\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7169801004288671)\n",
      "item: ('450.75 < textLength <= 900.50', -0.08150492331401075)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.08122896546039966)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07354854312979696)\n",
      "Essay: 8 / 50\n",
      "Sample: 461\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7447935794421807)\n",
      "item: ('writingQuality <= 450.75', -0.4340952929291134)\n",
      "item: ('argumentation <= 450.75', -0.2646825871109297)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.12127668153874432)\n",
      "Essay: 9 / 50\n",
      "Sample: 845\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7306817522665652)\n",
      "item: ('argumentation <= 450.75', -0.3231950034005105)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.14280016435991308)\n",
      "item: ('450.75 < textLength <= 900.50', -0.016025077885359288)\n",
      "Essay: 10 / 50\n",
      "Sample: 497\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.709272737015119)\n",
      "item: ('textLength <= 450.75', -0.20621631402879884)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.11884911131181033)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07210088463348927)\n",
      "Essay: 11 / 50\n",
      "Sample: 1174\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.720203582435964)\n",
      "item: ('writingQuality > 1350.25', 0.4068616705100056)\n",
      "item: ('textLength > 1350.25', 0.2446477828443295)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.08115118317078954)\n",
      "Essay: 12 / 50\n",
      "Sample: 1053\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7275882335824273)\n",
      "item: ('argumentation <= 450.75', -0.30231269294416024)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.16551297590043748)\n",
      "item: ('450.75 < textLength <= 900.50', -0.10116343161980458)\n",
      "Essay: 13 / 50\n",
      "Sample: 115\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('writingQuality <= 450.75', -0.4633742891494815)\n",
      "item: ('argumentation <= 450.75', -0.33183887485683317)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.16604292955904434)\n",
      "item: ('450.75 < textLength <= 900.50', -0.1241213930818831)\n",
      "Essay: 14 / 50\n",
      "Sample: 1355\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7510106594991937)\n",
      "item: ('argumentation <= 450.75', -0.30473035589880654)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.17778631680833448)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.09755834677009223)\n",
      "Essay: 15 / 50\n",
      "Sample: 913\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('argumentation <= 450.75', -0.34473313527664096)\n",
      "item: ('textLength <= 450.75', -0.2604245983544588)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.14461143806481175)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.1400897817833434)\n",
      "Essay: 16 / 50\n",
      "Sample: 1028\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('writingQuality <= 450.75', -0.44712835015620733)\n",
      "item: ('argumentation <= 450.75', -0.27841898705239837)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.14795849729162686)\n",
      "item: ('450.75 < textLength <= 900.50', -0.05559996677149677)\n",
      "Essay: 17 / 50\n",
      "Sample: 993\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.3556813451044558)\n",
      "item: ('argumentation <= 450.75', -0.3153239516339659)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.27939729785229866)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.08472020486690664)\n",
      "Essay: 18 / 50\n",
      "Sample: 90\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7349591612083799)\n",
      "item: ('argumentation <= 450.75', -0.27951454497007644)\n",
      "item: ('textLength > 1350.25', 0.22471014338891557)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.1767421590952966)\n",
      "Essay: 19 / 50\n",
      "Sample: 780\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.3144009791690035)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.18047649607523839)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.1067629518218652)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.07482207955531668)\n",
      "Essay: 20 / 50\n",
      "Sample: 1686\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7257473493610394)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.13774014892270425)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.13150582753089957)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07832184413944124)\n",
      "Essay: 21 / 50\n",
      "Sample: 414\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.18607493683494028)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.15360369327795897)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.12174164152077967)\n",
      "item: ('450.75 < textLength <= 900.50', -0.06345765146829109)\n",
      "Essay: 22 / 50\n",
      "Sample: 1189\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7285033486628757)\n",
      "item: ('writingQuality <= 450.75', -0.43353412070019814)\n",
      "item: ('450.75 < textLength <= 900.50', -0.08028348438730878)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.058858279871071985)\n",
      "Essay: 23 / 50\n",
      "Sample: 273\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.712752148473768)\n",
      "item: ('writingQuality <= 450.75', -0.44362096000419204)\n",
      "item: ('textLength > 1350.25', 0.22133182132981555)\n",
      "item: ('900.50 < argumentation <= 1350.25', 0.10941866491203658)\n",
      "Essay: 24 / 50\n",
      "Sample: 1286\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7400003689499488)\n",
      "item: ('writingQuality <= 450.75', -0.46801177517227965)\n",
      "item: ('textLength <= 450.75', -0.23022884508902564)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.0791960202579519)\n",
      "Essay: 25 / 50\n",
      "Sample: 1346\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7290889922108473)\n",
      "item: ('argumentation <= 450.75', -0.2998329641146403)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.08634503072718604)\n",
      "item: ('450.75 < textLength <= 900.50', -0.062383263903604005)\n",
      "Essay: 26 / 50\n",
      "Sample: 503\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('writingQuality > 1350.25', 0.38505898689437074)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.19684452039933542)\n",
      "item: ('900.50 < argumentation <= 1350.25', 0.13057486797457013)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.07326605403924541)\n",
      "Essay: 27 / 50\n",
      "Sample: 1409\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7419225379967349)\n",
      "item: ('writingQuality <= 450.75', -0.422107876792704)\n",
      "item: ('argumentation <= 450.75', -0.3037014624448289)\n",
      "item: ('textLength <= 450.75', -0.2446344519555257)\n",
      "Essay: 28 / 50\n",
      "Sample: 1122\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7792834371711022)\n",
      "item: ('writingQuality <= 450.75', -0.4440051563124267)\n",
      "item: ('textLength <= 450.75', -0.2860075785185913)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.06487647976089499)\n",
      "Essay: 29 / 50\n",
      "Sample: 382\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('textLength <= 450.75', -0.23565385772650352)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.19226288255407634)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.15322594338428705)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.08378897218354711)\n",
      "Essay: 30 / 50\n",
      "Sample: 1141\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7213062998810459)\n",
      "item: ('writingQuality <= 450.75', -0.4462078777111688)\n",
      "item: ('argumentation <= 450.75', -0.2862090798233444)\n",
      "item: ('textLength <= 450.75', -0.21228529510071362)\n",
      "Essay: 31 / 50\n",
      "Sample: 268\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7327871485086899)\n",
      "item: ('writingQuality <= 450.75', -0.4241415792192632)\n",
      "item: ('argumentation <= 450.75', -0.3125434864787529)\n",
      "item: ('450.75 < textLength <= 900.50', -0.09273018724868011)\n",
      "Essay: 32 / 50\n",
      "Sample: 485\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('argumentation <= 450.75', -0.33306933477597306)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.25142503165438934)\n",
      "item: ('textLength <= 450.75', -0.2211540589623841)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.13962028442709232)\n",
      "Essay: 33 / 50\n",
      "Sample: 1537\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.6922150999104411)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.11202296853926702)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07542438249573089)\n",
      "item: ('450.75 < textLength <= 900.50', -0.05562976115467562)\n",
      "Essay: 34 / 50\n",
      "Sample: 393\n",
      "157/157 [==============================] - 0s 1ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7551931555338468)\n",
      "item: ('writingQuality <= 450.75', -0.443676918991472)\n",
      "item: ('450.75 < textLength <= 900.50', -0.06737618926455323)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.05593377824826841)\n",
      "Essay: 35 / 50\n",
      "Sample: 1785\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7468523213319336)\n",
      "item: ('writingQuality <= 450.75', -0.41829364771990313)\n",
      "item: ('argumentation <= 450.75', -0.29525605684990924)\n",
      "item: ('450.75 < textLength <= 900.50', -0.07719768807631167)\n",
      "Essay: 36 / 50\n",
      "Sample: 1417\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7295286215567081)\n",
      "item: ('writingQuality <= 450.75', -0.4273189979545701)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.07324895874858431)\n",
      "item: ('450.75 < textLength <= 900.50', -0.060570121848538556)\n",
      "Essay: 37 / 50\n",
      "Sample: 1700\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.3868408238338405)\n",
      "item: ('argumentation <= 450.75', -0.3063170751990656)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.2371519670002733)\n",
      "item: ('textLength <= 450.75', -0.2268136029683268)\n",
      "Essay: 38 / 50\n",
      "Sample: 1099\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7566413306944255)\n",
      "item: ('writingQuality <= 450.75', -0.4430800003946592)\n",
      "item: ('450.75 < textLength <= 900.50', -0.09437194316206764)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.05272870814235257)\n",
      "Essay: 39 / 50\n",
      "Sample: 170\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7313282398104501)\n",
      "item: ('textLength <= 450.75', -0.2321879033052729)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.13138457922841684)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.11734155367615326)\n",
      "Essay: 40 / 50\n",
      "Sample: 435\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7268722013158518)\n",
      "item: ('argumentation <= 450.75', -0.29569483395538004)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.1025229657290777)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.09556029744298701)\n",
      "Essay: 41 / 50\n",
      "Sample: 1211\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7540027343982594)\n",
      "item: ('writingQuality <= 450.75', -0.41456232529112746)\n",
      "item: ('450.75 < textLength <= 900.50', -0.11681650263839576)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.035143848082221166)\n",
      "Essay: 42 / 50\n",
      "Sample: 1547\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7111652758512571)\n",
      "item: ('argumentation <= 450.75', -0.28983704806569255)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.11775289101755758)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.07927971877636247)\n",
      "Essay: 43 / 50\n",
      "Sample: 104\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.3830371666030735)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.24476566618676385)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.09183365173544375)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.08302369165697601)\n",
      "Essay: 44 / 50\n",
      "Sample: 872\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.7228653930682167)\n",
      "item: ('writingQuality <= 450.75', -0.39873657056776457)\n",
      "item: ('argumentation <= 450.75', -0.3028258082717397)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.1150791266665003)\n",
      "Essay: 45 / 50\n",
      "Sample: 963\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('wordUsage <= 450.75', -0.6915888800892579)\n",
      "item: ('argumentation <= 450.75', -0.3084034598948095)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.13481907978414662)\n",
      "item: ('900.50 < writingQuality <= 1350.25', 0.13124307094227622)\n",
      "Essay: 46 / 50\n",
      "Sample: 859\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.37534467028821805)\n",
      "item: ('argumentation <= 450.75', -0.27691783838731054)\n",
      "item: ('900.50 < wordUsage <= 1350.25', 0.23985619183602003)\n",
      "item: ('textLength <= 450.75', -0.2326730766303532)\n",
      "Essay: 47 / 50\n",
      "Sample: 1070\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('argumentation <= 450.75', -0.34135383583129547)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.19130710765300638)\n",
      "item: ('450.75 < writingQuality <= 900.50', -0.11652713522698539)\n",
      "item: ('450.75 < textLength <= 900.50', -0.08092274172818933)\n",
      "Essay: 48 / 50\n",
      "Sample: 667\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.35573833605197275)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.18507726772266267)\n",
      "item: ('450.75 < argumentation <= 900.50', -0.08165461502027262)\n",
      "item: ('900.50 < textLength <= 1350.25', 0.06786118247590442)\n",
      "Essay: 49 / 50\n",
      "Sample: 830\n",
      "157/157 [==============================] - 0s 2ms/step\n",
      "item: ('writingQuality > 1350.25', 0.326440983475948)\n",
      "item: ('argumentation <= 450.75', -0.27770559050880156)\n",
      "item: ('450.75 < wordUsage <= 900.50', -0.1848107723919241)\n",
      "item: ('450.75 < textLength <= 900.50', -0.09176521850165788)\n"
     ]
    }
   ],
   "source": [
    "#Save results\n",
    "sample_instance_index = 0\n",
    "pattern = r'[<>]'\n",
    "scoreDf = pd.read_csv('./Data/scoreDf.csv')\n",
    "\n",
    "d = {'Index': np.repeat(-1, len(X)), 'wordUsage': -1, 'writingQuality': -1, 'textLength': -1, 'argumentation': -1}\n",
    "limeDf = pd.DataFrame(data=d)\n",
    "\n",
    "i_ = 0\n",
    "for sample_instance_index in range(len(X)):\n",
    "    print('Essay:', i_, '/', len(X))\n",
    "\n",
    "    sample_instance = X.iloc[sample_instance_index][Xtrain.columns[4:]]\n",
    "    print('Sample:', X.iloc[sample_instance_index]['Index'])\n",
    "    text = X.iloc[sample_instance_index]['Text']\n",
    "    trackID = X.iloc[sample_instance_index]['Index']\n",
    "    \n",
    "    exp = explainer.explain_instance(sample_instance, predict_fn, num_features=len(Xtrain.columns[4:]), num_samples=5000)\n",
    "    expList = exp.as_list()\n",
    "   \n",
    "    #Create Overview\n",
    "    wordUsageScs = []\n",
    "    writingQualityScs = []\n",
    "    textLengthScs = []\n",
    "    argumentationScs = []\n",
    "\n",
    "    for item in expList:\n",
    "        print('item:', item)\n",
    "        category = str(item).split('<')\n",
    "        if len(category) == 2:\n",
    "            category = category[0].strip()[2:]\n",
    "        elif len(category) == 3:\n",
    "            category = category[1].strip()\n",
    "        else:\n",
    "            category = category[0].split('>')[0][2:].strip()\n",
    "        \n",
    "        if category == 'wordUsage':\n",
    "            wordUsageScs.append(item[1])\n",
    "        elif category == 'writingQuality':\n",
    "            writingQualityScs.append(item[1])\n",
    "        elif category == 'textLength':\n",
    "            textLengthScs.append(item[1])\n",
    "        elif category in 'argumentation':\n",
    "            argumentationScs.append(item[1])\n",
    "        else:\n",
    "            print('Wrong:', category)\n",
    "    \n",
    "    limeDf.at[i_, 'Index'] = trackID\n",
    "    limeDf.at[i_, 'wordUsage'] = sum(wordUsageScs)\n",
    "    limeDf.at[i_, 'writingQuality'] = sum(writingQualityScs)\n",
    "    limeDf.at[i_, 'textLength'] = sum(textLengthScs)\n",
    "    limeDf.at[i_, 'argumentation'] = sum(argumentationScs)\n",
    "    i_+= 1\n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>wordUsage</th>\n",
       "      <th>writingQuality</th>\n",
       "      <th>textLength</th>\n",
       "      <th>argumentation</th>\n",
       "      <th>positive_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>575</td>\n",
       "      <td>-0.186862</td>\n",
       "      <td>0.398257</td>\n",
       "      <td>0.225379</td>\n",
       "      <td>-0.104884</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512</td>\n",
       "      <td>0.266117</td>\n",
       "      <td>0.398605</td>\n",
       "      <td>-0.256597</td>\n",
       "      <td>-0.304213</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>896</td>\n",
       "      <td>-0.716179</td>\n",
       "      <td>-0.430923</td>\n",
       "      <td>-0.114690</td>\n",
       "      <td>-0.076982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1349</td>\n",
       "      <td>-0.746971</td>\n",
       "      <td>0.156862</td>\n",
       "      <td>-0.291165</td>\n",
       "      <td>-0.069986</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1795</td>\n",
       "      <td>-0.723339</td>\n",
       "      <td>-0.430234</td>\n",
       "      <td>-0.107689</td>\n",
       "      <td>-0.295101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>438</td>\n",
       "      <td>-0.190610</td>\n",
       "      <td>-0.066515</td>\n",
       "      <td>-0.082003</td>\n",
       "      <td>-0.328384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1178</td>\n",
       "      <td>-0.182588</td>\n",
       "      <td>0.413589</td>\n",
       "      <td>-0.061156</td>\n",
       "      <td>-0.052503</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1468</td>\n",
       "      <td>-0.716980</td>\n",
       "      <td>-0.081229</td>\n",
       "      <td>-0.081505</td>\n",
       "      <td>-0.073549</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>461</td>\n",
       "      <td>-0.744794</td>\n",
       "      <td>-0.434095</td>\n",
       "      <td>0.121277</td>\n",
       "      <td>-0.264683</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>845</td>\n",
       "      <td>-0.730682</td>\n",
       "      <td>0.142800</td>\n",
       "      <td>-0.016025</td>\n",
       "      <td>-0.323195</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>497</td>\n",
       "      <td>-0.709273</td>\n",
       "      <td>0.118849</td>\n",
       "      <td>-0.206216</td>\n",
       "      <td>-0.072101</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1174</td>\n",
       "      <td>-0.720204</td>\n",
       "      <td>0.406862</td>\n",
       "      <td>0.244648</td>\n",
       "      <td>-0.081151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1053</td>\n",
       "      <td>-0.727588</td>\n",
       "      <td>0.165513</td>\n",
       "      <td>-0.101163</td>\n",
       "      <td>-0.302313</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115</td>\n",
       "      <td>-0.166043</td>\n",
       "      <td>-0.463374</td>\n",
       "      <td>-0.124121</td>\n",
       "      <td>-0.331839</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1355</td>\n",
       "      <td>-0.751011</td>\n",
       "      <td>0.177786</td>\n",
       "      <td>0.097558</td>\n",
       "      <td>-0.304730</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>913</td>\n",
       "      <td>-0.144611</td>\n",
       "      <td>0.140090</td>\n",
       "      <td>-0.260425</td>\n",
       "      <td>-0.344733</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1028</td>\n",
       "      <td>-0.147958</td>\n",
       "      <td>-0.447128</td>\n",
       "      <td>-0.055600</td>\n",
       "      <td>-0.278419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>993</td>\n",
       "      <td>0.279397</td>\n",
       "      <td>0.355681</td>\n",
       "      <td>0.084720</td>\n",
       "      <td>-0.315324</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>90</td>\n",
       "      <td>-0.734959</td>\n",
       "      <td>0.176742</td>\n",
       "      <td>0.224710</td>\n",
       "      <td>-0.279515</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>780</td>\n",
       "      <td>-0.180476</td>\n",
       "      <td>0.314401</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>-0.106763</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1686</td>\n",
       "      <td>-0.725747</td>\n",
       "      <td>-0.137740</td>\n",
       "      <td>0.131506</td>\n",
       "      <td>-0.078322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>414</td>\n",
       "      <td>-0.153604</td>\n",
       "      <td>0.186075</td>\n",
       "      <td>-0.063458</td>\n",
       "      <td>-0.121742</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1189</td>\n",
       "      <td>-0.728503</td>\n",
       "      <td>-0.433534</td>\n",
       "      <td>-0.080283</td>\n",
       "      <td>-0.058858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>273</td>\n",
       "      <td>-0.712752</td>\n",
       "      <td>-0.443621</td>\n",
       "      <td>0.221332</td>\n",
       "      <td>0.109419</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1286</td>\n",
       "      <td>-0.740000</td>\n",
       "      <td>-0.468012</td>\n",
       "      <td>-0.230229</td>\n",
       "      <td>-0.079196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1346</td>\n",
       "      <td>-0.729089</td>\n",
       "      <td>-0.086345</td>\n",
       "      <td>-0.062383</td>\n",
       "      <td>-0.299833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>503</td>\n",
       "      <td>-0.196845</td>\n",
       "      <td>0.385059</td>\n",
       "      <td>0.073266</td>\n",
       "      <td>0.130575</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1409</td>\n",
       "      <td>-0.741923</td>\n",
       "      <td>-0.422108</td>\n",
       "      <td>-0.244634</td>\n",
       "      <td>-0.303701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1122</td>\n",
       "      <td>-0.779283</td>\n",
       "      <td>-0.444005</td>\n",
       "      <td>-0.286008</td>\n",
       "      <td>-0.064876</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>382</td>\n",
       "      <td>-0.192263</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>-0.235654</td>\n",
       "      <td>-0.083789</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1141</td>\n",
       "      <td>-0.721306</td>\n",
       "      <td>-0.446208</td>\n",
       "      <td>-0.212285</td>\n",
       "      <td>-0.286209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>268</td>\n",
       "      <td>-0.732787</td>\n",
       "      <td>-0.424142</td>\n",
       "      <td>-0.092730</td>\n",
       "      <td>-0.312543</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>485</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.139620</td>\n",
       "      <td>-0.221154</td>\n",
       "      <td>-0.333069</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1537</td>\n",
       "      <td>-0.692215</td>\n",
       "      <td>-0.112023</td>\n",
       "      <td>-0.055630</td>\n",
       "      <td>-0.075424</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>393</td>\n",
       "      <td>-0.755193</td>\n",
       "      <td>-0.443677</td>\n",
       "      <td>-0.067376</td>\n",
       "      <td>-0.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1785</td>\n",
       "      <td>-0.746852</td>\n",
       "      <td>-0.418294</td>\n",
       "      <td>-0.077198</td>\n",
       "      <td>-0.295256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1417</td>\n",
       "      <td>-0.729529</td>\n",
       "      <td>-0.427319</td>\n",
       "      <td>-0.060570</td>\n",
       "      <td>-0.073249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1700</td>\n",
       "      <td>0.237152</td>\n",
       "      <td>0.386841</td>\n",
       "      <td>-0.226814</td>\n",
       "      <td>-0.306317</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1099</td>\n",
       "      <td>-0.756641</td>\n",
       "      <td>-0.443080</td>\n",
       "      <td>-0.094372</td>\n",
       "      <td>-0.052729</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>170</td>\n",
       "      <td>-0.731328</td>\n",
       "      <td>0.131385</td>\n",
       "      <td>-0.232188</td>\n",
       "      <td>-0.117342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>435</td>\n",
       "      <td>-0.726872</td>\n",
       "      <td>-0.102523</td>\n",
       "      <td>0.095560</td>\n",
       "      <td>-0.295695</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1211</td>\n",
       "      <td>-0.754003</td>\n",
       "      <td>-0.414562</td>\n",
       "      <td>-0.116817</td>\n",
       "      <td>-0.035144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1547</td>\n",
       "      <td>-0.711165</td>\n",
       "      <td>-0.079280</td>\n",
       "      <td>0.117753</td>\n",
       "      <td>-0.289837</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>104</td>\n",
       "      <td>0.244766</td>\n",
       "      <td>0.383037</td>\n",
       "      <td>0.091834</td>\n",
       "      <td>-0.083024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>872</td>\n",
       "      <td>-0.722865</td>\n",
       "      <td>-0.398737</td>\n",
       "      <td>0.115079</td>\n",
       "      <td>-0.302826</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>963</td>\n",
       "      <td>-0.691589</td>\n",
       "      <td>0.131243</td>\n",
       "      <td>0.134819</td>\n",
       "      <td>-0.308403</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>859</td>\n",
       "      <td>0.239856</td>\n",
       "      <td>0.375345</td>\n",
       "      <td>-0.232673</td>\n",
       "      <td>-0.276918</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1070</td>\n",
       "      <td>-0.191307</td>\n",
       "      <td>-0.116527</td>\n",
       "      <td>-0.080923</td>\n",
       "      <td>-0.341354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>667</td>\n",
       "      <td>-0.185077</td>\n",
       "      <td>0.355738</td>\n",
       "      <td>0.067861</td>\n",
       "      <td>-0.081655</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>830</td>\n",
       "      <td>-0.184811</td>\n",
       "      <td>0.326441</td>\n",
       "      <td>-0.091765</td>\n",
       "      <td>-0.277706</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Index  wordUsage  writingQuality  textLength  argumentation  \\\n",
       "0     575  -0.186862        0.398257    0.225379      -0.104884   \n",
       "1    1512   0.266117        0.398605   -0.256597      -0.304213   \n",
       "2     896  -0.716179       -0.430923   -0.114690      -0.076982   \n",
       "3    1349  -0.746971        0.156862   -0.291165      -0.069986   \n",
       "4    1795  -0.723339       -0.430234   -0.107689      -0.295101   \n",
       "5     438  -0.190610       -0.066515   -0.082003      -0.328384   \n",
       "6    1178  -0.182588        0.413589   -0.061156      -0.052503   \n",
       "7    1468  -0.716980       -0.081229   -0.081505      -0.073549   \n",
       "8     461  -0.744794       -0.434095    0.121277      -0.264683   \n",
       "9     845  -0.730682        0.142800   -0.016025      -0.323195   \n",
       "10    497  -0.709273        0.118849   -0.206216      -0.072101   \n",
       "11   1174  -0.720204        0.406862    0.244648      -0.081151   \n",
       "12   1053  -0.727588        0.165513   -0.101163      -0.302313   \n",
       "13    115  -0.166043       -0.463374   -0.124121      -0.331839   \n",
       "14   1355  -0.751011        0.177786    0.097558      -0.304730   \n",
       "15    913  -0.144611        0.140090   -0.260425      -0.344733   \n",
       "16   1028  -0.147958       -0.447128   -0.055600      -0.278419   \n",
       "17    993   0.279397        0.355681    0.084720      -0.315324   \n",
       "18     90  -0.734959        0.176742    0.224710      -0.279515   \n",
       "19    780  -0.180476        0.314401    0.074822      -0.106763   \n",
       "20   1686  -0.725747       -0.137740    0.131506      -0.078322   \n",
       "21    414  -0.153604        0.186075   -0.063458      -0.121742   \n",
       "22   1189  -0.728503       -0.433534   -0.080283      -0.058858   \n",
       "23    273  -0.712752       -0.443621    0.221332       0.109419   \n",
       "24   1286  -0.740000       -0.468012   -0.230229      -0.079196   \n",
       "25   1346  -0.729089       -0.086345   -0.062383      -0.299833   \n",
       "26    503  -0.196845        0.385059    0.073266       0.130575   \n",
       "27   1409  -0.741923       -0.422108   -0.244634      -0.303701   \n",
       "28   1122  -0.779283       -0.444005   -0.286008      -0.064876   \n",
       "29    382  -0.192263        0.153226   -0.235654      -0.083789   \n",
       "30   1141  -0.721306       -0.446208   -0.212285      -0.286209   \n",
       "31    268  -0.732787       -0.424142   -0.092730      -0.312543   \n",
       "32    485   0.251425        0.139620   -0.221154      -0.333069   \n",
       "33   1537  -0.692215       -0.112023   -0.055630      -0.075424   \n",
       "34    393  -0.755193       -0.443677   -0.067376      -0.055934   \n",
       "35   1785  -0.746852       -0.418294   -0.077198      -0.295256   \n",
       "36   1417  -0.729529       -0.427319   -0.060570      -0.073249   \n",
       "37   1700   0.237152        0.386841   -0.226814      -0.306317   \n",
       "38   1099  -0.756641       -0.443080   -0.094372      -0.052729   \n",
       "39    170  -0.731328        0.131385   -0.232188      -0.117342   \n",
       "40    435  -0.726872       -0.102523    0.095560      -0.295695   \n",
       "41   1211  -0.754003       -0.414562   -0.116817      -0.035144   \n",
       "42   1547  -0.711165       -0.079280    0.117753      -0.289837   \n",
       "43    104   0.244766        0.383037    0.091834      -0.083024   \n",
       "44    872  -0.722865       -0.398737    0.115079      -0.302826   \n",
       "45    963  -0.691589        0.131243    0.134819      -0.308403   \n",
       "46    859   0.239856        0.375345   -0.232673      -0.276918   \n",
       "47   1070  -0.191307       -0.116527   -0.080923      -0.341354   \n",
       "48    667  -0.185077        0.355738    0.067861      -0.081655   \n",
       "49    830  -0.184811        0.326441   -0.091765      -0.277706   \n",
       "\n",
       "    positive_count  \n",
       "0                3  \n",
       "1                3  \n",
       "2                0  \n",
       "3                2  \n",
       "4                0  \n",
       "5                0  \n",
       "6                2  \n",
       "7                0  \n",
       "8                2  \n",
       "9                2  \n",
       "10               2  \n",
       "11               3  \n",
       "12               2  \n",
       "13               0  \n",
       "14               3  \n",
       "15               2  \n",
       "16               0  \n",
       "17               4  \n",
       "18               3  \n",
       "19               3  \n",
       "20               2  \n",
       "21               2  \n",
       "22               0  \n",
       "23               3  \n",
       "24               0  \n",
       "25               0  \n",
       "26               4  \n",
       "27               0  \n",
       "28               0  \n",
       "29               2  \n",
       "30               0  \n",
       "31               0  \n",
       "32               3  \n",
       "33               0  \n",
       "34               0  \n",
       "35               0  \n",
       "36               0  \n",
       "37               3  \n",
       "38               0  \n",
       "39               2  \n",
       "40               2  \n",
       "41               0  \n",
       "42               2  \n",
       "43               4  \n",
       "44               2  \n",
       "45               3  \n",
       "46               3  \n",
       "47               0  \n",
       "48               3  \n",
       "49               2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limeDf.to_csv('./Data/limeDf.csv', index=False)\n",
    "\n",
    "def count_positive_values(row):\n",
    "    return sum(1 for value in row if value > 0)\n",
    "\n",
    "limeDf['positive_count'] = limeDf.apply(count_positive_values, axis=1)\n",
    "limeDf['positive_count'] = limeDf['positive_count'] -1\n",
    "limeDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('writingQuality > 1350.25', 0.326440983475948),\n",
       " ('argumentation <= 450.75', -0.27770559050880156),\n",
       " ('450.75 < wordUsage <= 900.50', -0.1848107723919241),\n",
       " ('450.75 < textLength <= 900.50', -0.09176521850165788)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expList = exp.as_list()\n",
    "expList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = X.duplicated(subset=['wordUsage', 'writingQuality', 'textLength', 'argumentation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
